{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "CNN-Emotion detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavipatil29/Emotion-Detection-/blob/main/CNN_Emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oywmbd3D-UCi"
      },
      "source": [
        "**CNN**\r\n",
        "\r\n",
        "Why cnn?\r\n",
        "CNNs are very effective in reducing the number of parameters without losing on the quality of models. Images have high dimensionality (as each pixel is considered as a feature) which suits the above described abilities of CNNs. CNN's are really effective for image classification as the concept of dimensionality reduction suits the huge number of parameters in an image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "6vNTH-WZ9og7",
        "outputId": "71d93f79-3db3-470a-aa60-fc0aaafd3886"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import keras as k\n",
        "data=pd.read_csv('../input/fer2013/fer2013.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "xTkgZRgk9ohg"
      },
      "source": [
        "train_set = data[(data.Usage == 'Training')] \n",
        "val_set = data[(data.Usage == 'PublicTest')]\n",
        "test_set = data[(data.Usage == 'PrivateTest')] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ud8QEFsf9ohm"
      },
      "source": [
        "y_train=train_set['emotion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ksmoKY6B9ohr"
      },
      "source": [
        "X_train = np.array(list(map(str.split, train_set.pixels)), np.float32)\n",
        "\n",
        "X_val = np.array(list(map(str.split, val_set.pixels)), np.float32)\n",
        "X_test = np.array(list(map(str.split, test_set.pixels)), np.float32) \n",
        "Y_val=val_set['emotion']\n",
        "y_test=test_set['emotion']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gL0qZKnR9oht"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1) \n",
        "X_val = X_val.reshape(X_val.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BQnaFV689ohu"
      },
      "source": [
        "from keras.utils import np_utils,to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8BjS0YGB9ohu",
        "outputId": "57af560f-092c-4ec7-a0be-9c16e62a46d5"
      },
      "source": [
        "len(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28709"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jlp7z9E-9ohw"
      },
      "source": [
        "y_train = to_categorical(y_train, 7)\n",
        "Y_val = to_categorical(Y_val, 7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CLELtVFn9ohx"
      },
      "source": [
        "#CNN\n",
        "cnn4 = Sequential() #Sequential model allows to build a model layer by layer\n",
        "\n",
        "#--------------------------------Input layer------------------------------------------# \n",
        "#COnvolution layer parametrs:\n",
        "#No of output filters: 32\n",
        "#kernel_size: specifying the height and width of the 2D convolution window : (3,3)\n",
        "#activation function : relu\n",
        "#input_shape : shape of image matrix : 48x48x1 : (Gray Scale Image)\n",
        "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1))) \n",
        "#Batch Normalization : It is used to normalize the output of the previous layers. The activations scale the input layer in normalization.\n",
        "#Using batch normalization learning becomes efficient also it can be used as regularization to avoid overfitting of the model.  \n",
        "cnn4.add(BatchNormalization())\n",
        "\n",
        "#--------------------------------Second layer------------------------------------------# \n",
        "cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu')) \n",
        "cnn4.add(BatchNormalization())\n",
        "cnn4.add(MaxPooling2D(pool_size=(2, 2))) #Down-sampling opertaion to reduce dimensionality of feature map\n",
        "cnn4.add(Dropout(0.25)) #regularization technique to reduce overfitting: dropouts randomly switches off some neurons in the network which forces the data to find new paths\n",
        "\n",
        "#--------------------------------Third layer------------------------------------------# \n",
        "cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "cnn4.add(BatchNormalization())\n",
        "cnn4.add(Dropout(0.25))\n",
        "\n",
        "#--------------------------------Fourth layer------------------------------------------# \n",
        "cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "cnn4.add(BatchNormalization())\n",
        "cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn4.add(Dropout(0.25))\n",
        "\n",
        "cnn4.add(Flatten()) #converts 3d vector to 1 d vector : Flatten serves as a connection between the convolution and dense layers.\n",
        "\n",
        "#--------------------------------Output layers------------------------------------------# \n",
        "#‘Dense’ is the layer type we will use in for our output layer. \n",
        "cnn4.add(Dense(128, activation='relu')) #Dense layer predict labels\n",
        "cnn4.add(BatchNormalization())\n",
        "cnn4.add(Dropout(0.5))\n",
        "\n",
        "cnn4.add(Dense(128, activation='relu'))\n",
        "cnn4.add(BatchNormalization())\n",
        "cnn4.add(Dropout(0.5))\n",
        "\n",
        "cnn4.add(Dense(7, activation='softmax'))        #FINAL PREDICTION\n",
        "#Softmax makes the output sum up to 1 so the output can be interpreted as probabilities."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YJDFLyAm9oh2"
      },
      "source": [
        "#Parameters:\n",
        "#1) The optimizer controls the learning rate. Optmizerused : 'adam'. The adam optimizer adjusts the learning rate throughout training.\n",
        "#2) Loss function used: ‘categorical_crossentropy\". A lower score indicates that the model is performing better.\n",
        "#3) To make things even easier to interpret, ‘accuracy’ metric is used to see the accuracy score on the validation set when we train the model.\n",
        "cnn4.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SvA_GUM59oh7",
        "outputId": "2bc425c7-b752-4dc4-ca6c-162fdbb3663d"
      },
      "source": [
        "#The batch size defines the number of samples that will be propagated through the network.(at a time)\n",
        "cnn4.fit(X_train,\n",
        "         y_train,\n",
        "         batch_size=120,\n",
        "         epochs=30,\n",
        "         validation_data=(X_val, Y_val)\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 28709 samples, validate on 3589 samples\n",
            "Epoch 1/30\n",
            "28709/28709 [==============================] - 13s 451us/step - loss: 2.1235 - accuracy: 0.2791 - val_loss: 1.5957 - val_accuracy: 0.3884\n",
            "Epoch 2/30\n",
            "28709/28709 [==============================] - 8s 281us/step - loss: 1.6200 - accuracy: 0.3925 - val_loss: 1.4164 - val_accuracy: 0.4533\n",
            "Epoch 3/30\n",
            "28709/28709 [==============================] - 8s 277us/step - loss: 1.4369 - accuracy: 0.4496 - val_loss: 1.4469 - val_accuracy: 0.4294\n",
            "Epoch 4/30\n",
            "28709/28709 [==============================] - 8s 281us/step - loss: 1.3534 - accuracy: 0.4808 - val_loss: 1.2959 - val_accuracy: 0.5130\n",
            "Epoch 5/30\n",
            "28709/28709 [==============================] - 8s 287us/step - loss: 1.3009 - accuracy: 0.5044 - val_loss: 1.2449 - val_accuracy: 0.5366\n",
            "Epoch 6/30\n",
            "28709/28709 [==============================] - 8s 283us/step - loss: 1.2498 - accuracy: 0.5257 - val_loss: 1.2049 - val_accuracy: 0.5380\n",
            "Epoch 7/30\n",
            "28709/28709 [==============================] - 8s 278us/step - loss: 1.2142 - accuracy: 0.5410 - val_loss: 1.2074 - val_accuracy: 0.5347\n",
            "Epoch 8/30\n",
            "28709/28709 [==============================] - 8s 289us/step - loss: 1.1779 - accuracy: 0.5579 - val_loss: 1.4447 - val_accuracy: 0.4829\n",
            "Epoch 9/30\n",
            "28709/28709 [==============================] - 8s 284us/step - loss: 1.1507 - accuracy: 0.5661 - val_loss: 1.2295 - val_accuracy: 0.5339\n",
            "Epoch 10/30\n",
            "28709/28709 [==============================] - 8s 282us/step - loss: 1.1208 - accuracy: 0.5794 - val_loss: 1.1304 - val_accuracy: 0.5729\n",
            "Epoch 11/30\n",
            "28709/28709 [==============================] - 8s 278us/step - loss: 1.0865 - accuracy: 0.5944 - val_loss: 1.1664 - val_accuracy: 0.5481\n",
            "Epoch 12/30\n",
            "28709/28709 [==============================] - 8s 296us/step - loss: 1.0580 - accuracy: 0.6036 - val_loss: 1.1031 - val_accuracy: 0.5826\n",
            "Epoch 13/30\n",
            "28709/28709 [==============================] - 8s 284us/step - loss: 1.0344 - accuracy: 0.6147 - val_loss: 1.1851 - val_accuracy: 0.5581\n",
            "Epoch 14/30\n",
            "28709/28709 [==============================] - 8s 283us/step - loss: 1.0065 - accuracy: 0.6251 - val_loss: 1.1133 - val_accuracy: 0.5904\n",
            "Epoch 15/30\n",
            "28709/28709 [==============================] - 8s 278us/step - loss: 0.9816 - accuracy: 0.6353 - val_loss: 1.1409 - val_accuracy: 0.5837\n",
            "Epoch 16/30\n",
            "28709/28709 [==============================] - 8s 287us/step - loss: 0.9541 - accuracy: 0.6496 - val_loss: 1.1072 - val_accuracy: 0.5893\n",
            "Epoch 17/30\n",
            "28709/28709 [==============================] - 8s 282us/step - loss: 0.9282 - accuracy: 0.6560 - val_loss: 1.1026 - val_accuracy: 0.5979\n",
            "Epoch 18/30\n",
            "28709/28709 [==============================] - 8s 281us/step - loss: 0.9032 - accuracy: 0.6649 - val_loss: 1.1086 - val_accuracy: 0.6018\n",
            "Epoch 19/30\n",
            "28709/28709 [==============================] - 8s 278us/step - loss: 0.8789 - accuracy: 0.6746 - val_loss: 1.0870 - val_accuracy: 0.6046\n",
            "Epoch 20/30\n",
            "28709/28709 [==============================] - 8s 287us/step - loss: 0.8564 - accuracy: 0.6862 - val_loss: 1.1917 - val_accuracy: 0.5701\n",
            "Epoch 21/30\n",
            "28709/28709 [==============================] - 8s 281us/step - loss: 0.8261 - accuracy: 0.6927 - val_loss: 1.1220 - val_accuracy: 0.6002\n",
            "Epoch 22/30\n",
            "28709/28709 [==============================] - 8s 278us/step - loss: 0.8394 - accuracy: 0.6901 - val_loss: 1.1262 - val_accuracy: 0.5974\n",
            "Epoch 23/30\n",
            "28709/28709 [==============================] - 8s 279us/step - loss: 0.7902 - accuracy: 0.7109 - val_loss: 1.1693 - val_accuracy: 0.6016\n",
            "Epoch 24/30\n",
            "28709/28709 [==============================] - 8s 287us/step - loss: 0.7998 - accuracy: 0.7087 - val_loss: 1.1193 - val_accuracy: 0.6088\n",
            "Epoch 25/30\n",
            "28709/28709 [==============================] - 8s 279us/step - loss: 0.7464 - accuracy: 0.7257 - val_loss: 1.1361 - val_accuracy: 0.6074\n",
            "Epoch 26/30\n",
            "28709/28709 [==============================] - 8s 276us/step - loss: 0.7273 - accuracy: 0.7351 - val_loss: 1.1405 - val_accuracy: 0.6119\n",
            "Epoch 27/30\n",
            "28709/28709 [==============================] - 8s 285us/step - loss: 0.7343 - accuracy: 0.7335 - val_loss: 1.1456 - val_accuracy: 0.6108\n",
            "Epoch 28/30\n",
            "28709/28709 [==============================] - 8s 287us/step - loss: 0.7042 - accuracy: 0.7446 - val_loss: 1.1306 - val_accuracy: 0.6091\n",
            "Epoch 29/30\n",
            "28709/28709 [==============================] - 8s 279us/step - loss: 0.6792 - accuracy: 0.7522 - val_loss: 1.1589 - val_accuracy: 0.6110\n",
            "Epoch 30/30\n",
            "28709/28709 [==============================] - 8s 277us/step - loss: 0.6798 - accuracy: 0.7545 - val_loss: 1.1562 - val_accuracy: 0.6133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f632d5d2320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCdZeCQLCnjS"
      },
      "source": [
        "Training accuracy : 75.45%\r\n",
        "\r\n",
        "Validation accuracy : 61.33%\r\n",
        "\r\n",
        "Thus, CNN gives higher accuracy than ML algorithms."
      ]
    }
  ]
}